\documentclass[11pt]{article}
%\usepackage[hscale=0.8,vscale=0.8]{geometry}
\usepackage[margin=0.6in]{geometry}
\usepackage{palatino,url,amssymb, proba, amsmath}
\usepackage{titlesec,mdframed}
\usepackage{dsfont} %Use for indicator function \mathds{1}
\usepackage[english]{babel} % Use for figures
\usepackage{graphicx, caption} % Use for figures
\usepackage{epsf} % Use (encapsulated) postscript image files
\usepackage[export]{adjustbox} % Adjust alignment of figures
\usepackage{grffile} %Allows special characters (inc spaces!) in file paths
\usepackage{url} %typeset URLs
\usepackage{fixltx2e} %Allows textsubscript
\usepackage{afterpage} % Force figures to appear on next page
\usepackage{float} % force renderer to display table at particular point
\usepackage{multirow} % merge rows in table
\usepackage[font={normalsize,it}]{caption}
\usepackage{subcaption} % subfigure - kinda like subplot

\restylefloat{table}% cf package float:= force renderer to display table at particular point

% Code Snippet
\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
  language=r,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\footnotesize\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\usepackage{setspace} % Switch-up line spacing on demand
\usepackage{rotating} % sideways image!
%\graphicspath{{articles/article1/}}


% Custom Section/subsection setup
\usepackage{sectsty}
\sectionfont{\fontsize{14}{17}\selectfont\nohang\centering}
\subsectionfont{\fontsize{13}{16}\selectfont}

%\titleformat{\section}[block]{\Large\bfseries\filcenter}{}{1em}{}
\renewcommand{\thesubsection}{\arabic{subsection}}
\renewcommand{\thesubsubsection}{(\alph{subsubsection})}
\makeatletter
\def\@seccntformat#1{\@ifundefined{#1@cntformat}%
   {\csname the#1\endcsname\quad}%       default
   {\csname #1@cntformat\endcsname}}%    enable individual control
\newcommand\section@cntformat{}
\makeatother

\newcommand\eqdef{\mathrel{\stackrel{\makebox[0pt]{\mbox{\normalfont\tiny !}}}{=}}} % Text over Equals
\newcommand\propmarkov{\mathrel{\stackrel{\makebox[0pt]{\mbox{\normalfont\tiny markov}}}{\propto}}} % Text over Equals
\newcommand{\eqname}[1]{\tag*{#1}}% Tag equation with name
\newcommand*\mean[1]{\overline{#1}}
\DeclareMathOperator*{\argmin}{argmin} % Allow argmin
\DeclareMathOperator*{\argmax}{arg\,max} % Allow argmax
\DeclareMathOperator*{\var}{Var} % Allow variance
\DeclareMathOperator*{\proj}{proj} % Allow projection
\DeclareMathOperator{\Trace}{Tr}
\def\*#1{\mathbf{#1}}
\def\bs{\boldsymbol}
\def\Tr{^\top}

\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{pmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{pmatrix}
        \fi
}

% Do not read the preamble of included docs
\usepackage{standalone}
\usepackage{fancyhdr}
\pagestyle{fancy}
\rhead{nectr}
\linespread{1.1} 
\begin{document}
\section*{Non-parametric Exploratory Clustering using TURN-RES (nectr)}
\section*{\today}
\hspace*{\fill}Alex Bird\hspace*{\fill}
\vspace*{20pt}

\subsection{MAP Estimation of Gaussian Mixture Model}
This document is not intended to be a tutorial on deriving the update equations for a Gaussian Mixture Model (GMM). However, we aim to be transparent with regard to the content of the package. While the algorithm for TURN-RES is available in (Foss, 2002 \cite{F02}), fitting the GMM is not done using the standard Maximum Likelihood update equations, but using a MAP estimate. The Bayesian machinery available in MAP estimates is particularly useful for this package, since the clusters discovered using TURN-RES can be used as priors in a sort of Empirical Bayes approach. As one might suspect, the MAP update equations are very similar to the ML equations, but the derivation may be helpful to some. Those not familiar with fitting latent variable models are invited to read Bishop \cite{B06}, chapter 9, while the free energy formulation can be found in Neal \& Hinton, 1998 \cite{N98}.

\subsubsection{The Free Energy for MAP estimation}
Free energy is defined below for observed $x$ and latent $z$ as a lower bound on the (log) likelihood:
$$ \mathcal{F}(q, \theta) := \int q(z) \log \frac{p(x, z|\theta)}{q(z)}dz $$
As a lower bound on MAP, up to a constant of proportionality:
$$ \mathcal{F}_{\text{\tiny{MAP}}}(q, \theta) := \int q(z) \log \frac{p(x, z|\theta) p(\theta)}{q(z)}dz $$
The E and M steps are quickly derived and shown to be similar to ML:
\begin{align*}
\mathcal{F}_{\text{\tiny{MAP}}}(q, \theta) &=  \int q(z) \log \frac{p(z| x, \theta) p(x |\theta) p(\theta)}{q(z)}dz\\[5pt]
&=  \int q(z) \log \frac{p(z| x, \theta)}{q(z)}dz + \int q(z) \log p(x |\theta) p(\theta) dz \\[10pt]
&=  KL\left(q(z)||p(z|x,\theta)\right) + \ell_{\text{\tiny{MAP}}}(\theta)
\end{align*}
Thus the E-step is maximised when we take $q(z)$ to be the posterior over the latents and the M-step is a maximisation over $\E_{q(z)} \left[p(x, z|\theta) p(\theta)\right]$
\\[20pt]
\subsubsection{GMM Model}
Likelihood term:
$$ p(x|\pi_{1:K}, \mu_{1:K}, \Sigma_{1:K}) = \sum_z \prod_{k=1}^K \pi_k^{z_k} \mathcal{N}(x|\mu_k, \Sigma_k)^{z_k}$$
$z$ is coded as a 1-of-K vector. We use conjugate priors to ensure tractable inference:
\begin{align*}
\begin{split}
&p(\bs{\pi}|\bs{\alpha}) = \text{Dir}(\bs{\alpha})\\
&p(\mu_k|\Sigma_k, \beta_k) = \mathcal{N}(\mu_k|m_k, \beta_k^{-1} \Sigma_k)\\
&p(\Sigma_k|\nu_k, \Psi_k) = \text{InvWish}(\Sigma_k|\Psi_k, \nu_k)
\end{split}
\begin{split}
\text{choose:} \qquad  &\alpha_k = \alpha_0 \tilde\pi_k \\
&m_k = \tilde\mu_k \\
&\Psi_k = \tilde\Sigma_k
\end{split}
\end{align*}
where $ \tilde\pi_k, \tilde\mu_k, \tilde\Sigma_k$ are the empirical estimates of the mixing proportions, means and covariances of each cluster. $\alpha_0, \beta_{1:K}, \nu_{1:K}$ are tuning parameters to adjust the strength of the prior. The joint distribution is:
\begin{align*}
p(x_{1:N}, z_{1:N}, \pi_{1:K}, \mu_{1:K}, \Sigma_{1:K}) & = \left(\prod_{k=1}^K  \left(\prod_{n=1}^N \pi_k^{z_{nk}} \mathcal{N}(x_n|\mu_k, \Sigma_k)^{z_{nk}}\right) \mathcal{N}(\mu_k|m_k, \beta_k^{-1} \Sigma_k)\text{InvWish}(\Sigma_k|\Psi_k, \nu_k)\right)   \text{Dir}(\bs{\alpha}) \\[20pt]
& \propto \prod_{k=1}^K  \left(\prod_{n=1}^N \pi_k^{z_{nk}}\left(|2\pi\Sigma_k|^{-\frac{1}{2}}\exp\left\{-\frac{1}{2}(x_n-\mu_k)\Tr \Sigma_k^{-1}(x_n-mu_k)\right\}  \right)^{z_{nk}}\right) \\
& \quad \times |2\pi\beta_k^{-1}\Sigma_k|^{-\frac{1}{2}}\exp\left\{-\frac{\beta_k}{2}(\mu_k-m_k)\Tr \Sigma_k^{-1}(\mu_k - m_k)\right\}\\
& \quad \times |\Sigma_k|^{-\frac{1}{2}(\nu_k + d + 1)} \exp\left\{-\frac{1}{2}\text{Tr}\left(\left(\nu_k \Psi_k\right)\Sigma_k^{-1}\right)\right\} \quad \pi_k^{\alpha_k - 1}
\end{align*}
Note the particular form of the Inverse Wishart, where the parameter $\nu_k$ also multiplies the prior matrix. This is chosen to result in a more interpretable choice of the pair $(\nu_k, \Psi_k)$
\subsubsection{E-step for MAP GMM}
This is identical to the E-step for maximum likelihood,
\begin{align*}
r_{nk} := q(z_{nk}) \;\: \propto \:\; \pi_k^{z_{nk}} \mathcal{N}(x_n|\mu_k, \Sigma_k)^{z_{nk}} \quad \Rightarrow \quad
r_{nk}=  \frac{\pi_k^{z_{nk}} \mathcal{N}(x_n|\mu_k, \Sigma_k)^{z_{nk}}}{\sum_{k=1}^K \pi_k^{z_{nk}} \mathcal{N}(x_n|\mu_k, \Sigma_k)^{z_{nk}}}
\end{align*}
\subsubsection{M-step for MAP GMM}
\underline{Estimate for $\hat\pi_k$}:\\
\begin{align*}
\log \E_z\left[ p(x, z, \pi)\right] &\propto \left( \sum_{n=1}^N r_{nk}\log\pi_k\right) + (\alpha_k-1)\log\pi_k\\
\mathcal{L}(\bs\pi, \lambda) :&= \sum_k (\alpha_k - 1 + \sum_n r_{nk})\log\pi_k \quad -\lambda(\sum_k \pi_k - 1)\\
\frac{\partial \mathcal{L}}{\partial \pi_k} &= \frac{N_k + \alpha_k - 1}{\pi_k} - \lambda \quad \eqdef  0 
\intertext{solving for $\lambda$ in the usual way gives:} \hat\pi_k &= \frac{N_k + \alpha_k - 1}{\sum_k (N_k + \alpha_k - 1)} = \frac{N_k + \alpha_k - 1}{N - K + \sum_k \alpha_k} \tag{1}\label{est_pi}
\end{align*}
where $N_k = \sum_n r_{nk}$. \\\\
\underline{Estimate for $\hat\mu_k$}:\\
\begin{align*}
\log \E_z \left[ p(x, z, \mu_k)\right] &\propto -\frac{1}{2}\left(\left(\sum_{n=1}^Nr_{nk}(x_n - \mu_k)\Tr\Sigma_k^{-1}(x_n - \mu_k) \right)+ \beta_k(\mu_k - m_k)\Tr\Sigma_k^{-1}(\mu_k - m_k)\right)\\
\frac{\partial}{\partial \mu_k} &= - \left(\sum_{n=1}^N r_{nk}\Sigma_k^{-1}  \mu_k - r_{nk}\Sigma_k^{-1}x_n \right) - \beta_k \left(\Sigma_k^{-1} \mu_k - \Sigma_k^{-1} m_k\right) \:\; \eqdef \:\; 0\\[10pt]
\Rightarrow \quad \hat\mu_k & = \frac{\sum_n r_{nk}x_n + \beta_k m_k}{N_k + \beta_k} \tag{2}\label{est_mu}
\end{align*}\\
\underline{Estimate for $\hat\Sigma_k$}:\\
\begin{align*}
\log \E_z \left[ p(x, z, \mu_k, \Sigma_k)\right] & \propto  -\frac{1}{2}\left(\left(\sum_{n=1}^N r_{nk}\log|2\pi\Sigma_k| + r_{nk}(x_n - \mu_k)\Tr\Sigma_k^{-1}(x_n - \mu_k) \right) \right.+\\
& \left. \qquad  \log|2\pi\beta_k^{-1}\Sigma_k| +  \beta_k(\mu_k - m_k)\Tr\Sigma_k^{-1}(\mu_k - m_k) + (\nu_k + d + 1)\log|\Sigma_k| + \Trace\left(\nu_k\Psi_k\Sigma_k^{-1}\right)
\right)\\
& \propto \left(N_k + \nu_k + d + 2\right) \log|\Sigma_k| + \Trace\left(\sum_n r_{nk} (x_n - \mu_k)(x_n-\mu_k)\Tr \Sigma_k^{-1}\right) +\\
& \qquad  \beta_k\Trace\left((\mu_k - m_k)(\mu_k - m_k)\Tr\Sigma_k^{-1}\right) + \Trace\left(\nu_k\Psi_k\Sigma_k^{-1}\right)\\[10pt]
\frac{\partial}{\partial \Sigma_k^{-1}} & = -\left(N_k + \nu_k + d + 2\right)\Sigma\Tr + \left(\sum_n r_{nk} (x_n - \mu_k)(x_n-\mu_k)\Tr + \beta_k(\mu_k - m_k)(\mu_k - m_k)\Tr + \nu_k\Psi_k\right)\\[10pt]
\Rightarrow \quad \hat\Sigma_k &=\frac{\sum_n r_{nk} (x_n - \mu_k)(x_n-\mu_k)\Tr + \beta_k(\mu_k - m_k)(\mu_k - m_k)\Tr + \nu_k\Psi_k}{N_k + \nu_k + d + 2} \tag{3}\label{est_sigma}
\end{align*}

\subsubsection{Notes on Parameter Choices}
We still have a number of hyperparameters to choose even though TURN-RES can give us estimates of the sufficient statistics. We will treat each in turn:\\
\begin{enumerate}
\item \textbf{Dirichlet distribution over $\bs\pi$}. We can match the first moments of the dirichlet with the ML estimates from TURN-RES. This gives us:
$$ \alpha_k = \frac{\sum_n \delta\left(x_n \in \text{cls}(k)\right)} {N}$$
ie. the empirical proportions discovered in the TURN-RES stage. However, this gives us only the mean of the prior - we must identify the \emph{strength} (or covariance) of the prior. One option would be to set a baseline minimum variance, set to avoid  sparse distributions given by $\alpha_k < 1$ for some $k$. Thus we choose $a = (\min_k \alpha_k)^{-1}$ and $\alpha_k = aN_k/N$. Another option might be to match the second moments and solve for a. Unfortunately this is non-trivial, not least because the covariance matrix of a Dirichlet is singular by construction. We implement the first option since it is not obvious that we always wish to reduce variance in $\pi$ with increasing dataset size, for instance due to multiscale issues with TURN-RES.
\item \textbf{Gaussian distribution over $\bs{\mu_k}$}. We are using a Normal Inverse Wishart prior, and thus the covariance of the means is the covariance of the component, $\Sigma_k$. However, while we cannot change the covariance per se, there is a hyperparameter $\beta_k$ that can be used to scale the covariance in line with our beliefs about $\mu_k$. Note the dependence on $\beta_k$ in (\ref{est_mu}) and (\ref{est_sigma}). In (\ref{est_mu}), we see that the mean estimator can be understood as a convex combination of the ML estimate and the prior. Taking $\beta_k \rightarrow 0$, it reduces to the ML estimate, and taking $\beta_k \rightarrow \infty$, only the prior influence remains. \par
We must be careful about the influence upon the covariance estimate however. It is clear from (\ref{est_sigma}) that the covariance can be increased arbitrarily with the parameter $\beta_k$ and thus it is inadvisable to make $\beta_k$ large. This is particularly unfortunate for us, since we want the facility to set means as given by TURN-RES, which is clearly not feasible by use of $\beta_k$.
\item \textbf{Inverse Wishart distribution over $\bs{\Sigma_k}$}: In an exponential family conjugate prior, we usually have two parameters to choose, the counts of pseudo-observations and the sufficient statistics of pseudo-observations. It is easy to see that these correspond to the parameters $\nu_k$ and $\Psi_k$. But since from the conjugate derivation, $\Psi_k = \tilde{n}\tilde{S}$, where $\tilde{n}$ and $\tilde{S}$ are the pseudo counts and pseudo scatter matrix respectively, we likewise must multiply the empirical scatter matrix $\tilde{\Sigma}_k = \sum_{n \in \text{cl}(k)} (x_n - \mu_k^{t-1})(x_n - \mu_k^{t-1})\Tr /N_k$ by the count. Hence the density given above has $\nu_k\Psi_k$ instead of simply $\Psi_k$ inside the Trace.\par
Since both $\beta_k$ and $\nu_k$ are `pseudo counts', or a measure of our beliefs regarding the priors of $\mu_k$ and $\Sigma_k$, it makes sense to couple them - set $\nu_k = \beta_k$. This also solves the problem associated with large values of $\beta_k$. Since this value is now on the denominator of the $\hat\Sigma_k$ estimate, we can take $\beta_k$ arbitrarily large. This does restrict the degrees of freedom in the model, and because we require $\nu_k \ge d+1$, where $d$ is the dimension of the model, we are restricted in our choice of $\beta_k$. Finally, the effective pseudo-sufficient statistic for $p(\Sigma_k)$ is now $(\mu_k - m_k)(\mu_k - m_k)\Tr + \Psi_k$. We believe this is a tolerable restriction since the additional term simply reflects the uncertainty in $\mu_k$ in the covariance estimate.
\end{enumerate}
This leaves only 2 types of free paramaters for the user to choose, $a$ (\texttt{alpha} in the \texttt{fitGMM} documentation), and $\beta_k$. Both reflect the strength of belief in our prior, that is the clusters found during the TURN-RES phase. And we can even choose $\beta_k \propto \alpha_k$, the empirical proportions from TURN-RES with an appropriate multiplier $b$, (\texttt{beta} in the \texttt{fitGMM} documentation). For ease, the user is only introduced to these \emph{strength} parameters, as a prior belief in the mixing proportions and a prior belief in the cluster moments respectively, but we leave access to the underlying parameters for the more adventurous users.
\vspace{20pt}

\subsubsection{Likelihood calculation}
\begin{align*}
p(x|\pi, \mu, \Sigma)p(\pi, \mu, \Sigma) &= \left( \prod_{n=1}^N \sum_z \prod_{k=1}^K \pi_k^{z_k} \mathcal{N}(x_n|\mu_k, \Sigma_k)^{z_{nk}}\right) \times\\
&\qquad \left(\prod_{k=1}^K \mathcal{N}(\mu_k|m_k, \beta_k^{-1} \Sigma_k)\text{InvWish}(\Sigma_k|\Psi_k, \nu_k)\right)  \text{Dir}(\bs{\alpha})\\[15pt]
\ell_{\text{\tiny{MAP}}}(\pi, \mu, \Sigma) & = \sum_{n=1}^N \log\left(\sum_z \pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k)\right) + \left( \sum_{k=1}^K \log\mathcal{N}(\mu_k|m_k, \beta_k^{-1} \Sigma_k)\right.\\
& \qquad \left. -\frac{\nu_k + d+ 1}{2}\log|\Sigma_k| - \frac{\nu_k}{2}\Trace\left(\Psi_k\Sigma_k^{-1}\right) + (\alpha_k-1)\log\pi_k  \right) + \text{const}
\end{align*}
An option has been added to skip evaluations, since often the extra iterations of EM performed before convergence is diagnosed are computationally less expensive than the likelihood evaluations that have been omitted.
\begin{thebibliography}{1}
\bibitem{F02}
{A. Foss, O.R. Zaiane,
\emph{A parameterless method for efficiently discovering clusters of arbitrary shape in large datasets}.\\
IEEE, 2002}
\bibitem{B06}
 {C. Bishop,
  \emph{Pattern Recognition and Machine Learning}.\\
  Springer, 2006}
  \bibitem{N98}
 {R.M. Neal, G.E. Hinton,
  \emph{A view of the EM algorithm that justifies incremental, sparse, and other variants}.\\
  Learning in Graphical Models, MIT Press, 1998}
\end{thebibliography}

\end{document}